{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot without classes incoporated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryPrecisionRecallCurve\n",
    "\n",
    "def clip_pred(imgs):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "        \n",
    "    \n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.),\n",
       " tensor(0.),\n",
       " tensor([[2., 0.],\n",
       "         [0., 0.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = '/Users/lucialu/Github/DTU_Workspace/5_semester/02456-deep-learning-with-PyTorch/4_Convolutional/images'\n",
    "images, labels = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images)\n",
    "metrics(probs[:,1], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC adjusted zero-shot no classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    \"\"\"\n",
    "    Load images and their corresponding labels from specified folders.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "    \n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "    \n",
    "    return images, torch.tensor(labels, dtype=torch.int)\n",
    "\n",
    "def evaluate_model(images, labels, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    \n",
    "    print(f\"Accuracy: {acc.compute().item():.4f}\")\n",
    "    print(f\"F1 Score: {f1.compute().item():.4f}\")\n",
    "    print(f\"Confusion Matrix: {cm.compute()}\")\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot with classes implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'automobile': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>],\n",
       " 'bird': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>],\n",
       " 'cat': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>],\n",
       " 'deer': [],\n",
       " 'dog': [],\n",
       " 'frog': [],\n",
       " 'horse': [],\n",
       " 'ship': [],\n",
       " 'truck': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>],\n",
       " 'airplane': []}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryPrecisionRecallCurve\n",
    "\n",
    "def clip_pred(imgs, imgs_class):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    logits = torch.empty(0,2)\n",
    "\n",
    "    for i in range(len(imgs)):\n",
    "        input = processor(\n",
    "            text=[\"an artificially generated image of a\" + str(imgs_class[i]), \"a real image of a\" + str(imgs_class[i])],\n",
    "            images=imgs[i],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        output = model(**input)\n",
    "        logits = torch.cat((logits, output.logits_per_image), dim=0)  # this is the image-text similarity score\n",
    "    prob = logits.softmax(dim=1)  \n",
    "    return prob\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)':'automobile','(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder): # SPLIT billeder op i hver deres class i stedet for at loop over alle enkelte billder\n",
    "        # Herefter kør modellen for hver class\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "                else:\n",
    "                    imgs_class.append('airplane')\n",
    "                    break\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def load_image_classes_from_folder(folder):\n",
    "    class_types = {'(2)': 'automobile', '(3)':'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    images_by_class = {i: [] for i in class_types.values()} \n",
    "    images_by_class['airplane'] = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "\n",
    "        for key, value in class_types.items():\n",
    "            if key in filename:\n",
    "                img_class = value\n",
    "                break\n",
    "            else:\n",
    "                img_class = 'airplane'\n",
    "\n",
    "        images_by_class[img_class].append(img)\n",
    "\n",
    "    return images_by_class\n",
    "\n",
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    prc = BinaryPrecisionRecallCurve()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    prc.update(ys, ts)\n",
    "\n",
    "    return acc.compute(), f1.compute(), cm.compute(), prc.compute()\n",
    "\n",
    "folder_path = \"images\"\n",
    "# images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "# probs = clip_pred(images, imgs_class)\n",
    "# metrics(probs[:,1], labels)\n",
    "\n",
    "images_by_class = load_image_classes_from_folder(folder_path)\n",
    "images_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'automobile': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>],\n",
       " 'bird': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>],\n",
       " 'cat': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>],\n",
       " 'deer': [],\n",
       " 'dog': [],\n",
       " 'frog': [],\n",
       " 'horse': [],\n",
       " 'ship': [],\n",
       " 'truck': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>],\n",
       " 'airplane': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_types = {'(2)': 'automobile', '(3)':'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "images_by_class = {i: [] for i in class_types.values()} \n",
    "images_by_class['airplane'] = []\n",
    "\n",
    "folder = 'images'\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    img = Image.open(os.path.join(folder, filename))\n",
    "\n",
    "    for key, value in class_types.items():\n",
    "        if key in filename:\n",
    "            img_index = value\n",
    "            break\n",
    "        else:\n",
    "            img_index = 'airplane'\n",
    "\n",
    "    images_by_class[img_index].append(img)\n",
    "\n",
    "images_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) automobile\n",
      "(3) bird\n",
      "(4) cat\n",
      "(5) deer\n",
      "(6) dog\n",
      "(7) frog\n",
      "(8) horse\n",
      "(9) ship\n",
      "(10) truck\n",
      "(2) automobile\n",
      "(3) bird\n",
      "(4) cat\n",
      "(5) deer\n",
      "(6) dog\n",
      "(7) frog\n",
      "(8) horse\n",
      "(9) ship\n",
      "(10) truck\n",
      "(2) automobile\n",
      "(3) bird\n",
      "(4) cat\n",
      "(5) deer\n",
      "(6) dog\n",
      "(7) frog\n",
      "(8) horse\n",
      "(9) ship\n",
      "(10) truck\n",
      "(2) automobile\n",
      "(3) bird\n",
      "(4) cat\n",
      "(5) deer\n",
      "(6) dog\n",
      "(7) frog\n",
      "(8) horse\n",
      "(9) ship\n",
      "(10) truck\n",
      "(2) automobile\n",
      "(3) bird\n",
      "(4) cat\n",
      "(5) deer\n",
      "(6) dog\n",
      "(7) frog\n",
      "(8) horse\n",
      "(9) ship\n",
      "(10) truck\n",
      "(2) automobile\n",
      "(3) bird\n",
      "(4) cat\n",
      "(5) deer\n",
      "(6) dog\n",
      "(7) frog\n",
      "(8) horse\n",
      "(9) ship\n",
      "(10) truck\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(folder):\n",
    "    img = Image.open(os.path.join(folder, filename))\n",
    "\n",
    "    for key, value in class_types.items():\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    \"\"\"\n",
    "    Load images and their corresponding labels from specified folders.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "    \n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "    \n",
    "    return images, torch.tensor(labels, dtype=torch.int)\n",
    "\n",
    "def evaluate_model(images, labels, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    \n",
    "    print(f\"Accuracy: {acc.compute().item():.4f}\")\n",
    "    print(f\"F1 Score: {f1.compute().item():.4f}\")\n",
    "    print(f\"Confusion Matrix: {cm.compute()}\")\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02456",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
