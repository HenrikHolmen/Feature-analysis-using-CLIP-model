{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot without classes incoporated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryPrecisionRecallCurve\n",
    "\n",
    "def clip_pred(imgs):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0036, 0.0354], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = clip_pred(images)\n",
    "probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "        \n",
    "    \n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.),\n",
       " tensor(0.),\n",
       " tensor([[2., 0.],\n",
       "         [0., 0.]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\DTU_repo\\deep_learning\\4_Convolutional\\images'\n",
    "images, labels = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images)\n",
    "metrics(probs[:,1], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC adjusted zero-shot no classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    \"\"\"\n",
    "    Load images and their corresponding labels from specified folders.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "    \n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "    \n",
    "    return images, torch.tensor(labels, dtype=torch.int)\n",
    "\n",
    "def evaluate_model(images, labels, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    \n",
    "    print(f\"Accuracy: {acc.compute().item():.4f}\")\n",
    "    print(f\"F1 Score: {f1.compute().item():.4f}\")\n",
    "    print(f\"Confusion Matrix: {cm.compute()}\")\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot with classes implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikto\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvikto\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDTU_repo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdeep_learning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4_Convolutional\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     70\u001b[0m images, labels, imgs_class \u001b[38;5;241m=\u001b[39m load_images_from_folder(folder_path)\n\u001b[1;32m---> 71\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mclip_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m metrics(probs[:,\u001b[38;5;241m1\u001b[39m], labels)\n",
      "Cell \u001b[1;32mIn[101], line 15\u001b[0m, in \u001b[0;36mclip_pred\u001b[1;34m(imgs, imgs_class)\u001b[0m\n\u001b[0;32m     11\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(imgs)):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m---> 15\u001b[0m         text\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man artificially generated image of a\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mimgs_class\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma real image of a\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(imgs_class[i])],\n\u001b[0;32m     16\u001b[0m         images\u001b[38;5;241m=\u001b[39mimgs[i],\n\u001b[0;32m     17\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     )\n\u001b[0;32m     21\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     22\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((logits, output\u001b[38;5;241m.\u001b[39mlogits_per_image), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# this is the image-text similarity score\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryPrecisionRecallCurve\n",
    "\n",
    "def clip_pred(imgs, imgs_class):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    logits = torch.empty(0,2)\n",
    "\n",
    "    for i in range(len(imgs)):\n",
    "        input = processor(\n",
    "            text=[\"an artificially generated image of a\" + str(imgs_class[i]), \"a real image of a\" + str(imgs_class[i])],\n",
    "            images=imgs[i],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        output = model(**input)\n",
    "        logits = torch.cat((logits, output.logits_per_image), dim=0)  # this is the image-text similarity score\n",
    "    prob = logits.softmax(dim=1)  \n",
    "    return prob\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)':'automobile','(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "                else:\n",
    "                    imgs_class.append('airplane')\n",
    "                    break\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    prc = BinaryPrecisionRecallCurve()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    prc.update(ys, ts)\n",
    "\n",
    "    return acc.compute(), f1.compute(), cm.compute(), prc.compute()\n",
    "\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\DTU_repo\\deep_learning\\4_Convolutional\\images'\n",
    "images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images, imgs_class)\n",
    "metrics(probs[:,1], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    \"\"\"\n",
    "    Load images and their corresponding labels from specified folders.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "    \n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "    \n",
    "    return images, torch.tensor(labels, dtype=torch.int)\n",
    "\n",
    "def evaluate_model(images, labels, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    \n",
    "    print(f\"Accuracy: {acc.compute().item():.4f}\")\n",
    "    print(f\"F1 Score: {f1.compute().item():.4f}\")\n",
    "    print(f\"Confusion Matrix: {cm.compute()}\")\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
