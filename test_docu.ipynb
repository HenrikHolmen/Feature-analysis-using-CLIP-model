{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot without classes incoporated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryPrecisionRecallCurve\n",
    "\n",
    "def clip_pred(imgs):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "        \n",
    "    \n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\DTU_repo\\deep_learning\\4_Convolutional\\images'\n",
    "images, labels = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images)\n",
    "metrics(probs[:,1], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC adjusted zero-shot no classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    \"\"\"\n",
    "    Load images and their corresponding labels from specified folders.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "    \n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "    \n",
    "    return images, torch.tensor(labels, dtype=torch.int)\n",
    "\n",
    "def evaluate_model(images, labels, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    \n",
    "    print(f\"Accuracy: {acc.compute().item():.4f}\")\n",
    "    print(f\"F1 Score: {f1.compute().item():.4f}\")\n",
    "    print(f\"Confusion Matrix: {cm.compute()}\")\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot with classes implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images_class = {'airplane': [[],[]], 'automobile': [[],[]], 'bird': [[],[]], 'cat': [[],[]], 'deer': [[],[]], 'dog': [[],[]], 'frog': [[],[]], 'horse': [[],[]], 'ship': [[],[]], 'truck': [[],[]]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types.get(c)][0].append(img)\n",
    "                    images_class[class_types.get(c)][1].append(0)\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)\n",
    "\n",
    "    return images_class\n",
    "\n",
    "folder = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "images_class = load_images_from_folder(folder)\n",
    "\n",
    "l = []\n",
    "for i in images_class:\n",
    "    l.extend(images_class[i][1])\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikto\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.13333334028720856\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: \n",
      "tensor([[ 4., 26.],\n",
      "        [ 0.,  0.]])\n",
      "AUROC: 0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "def clip_pred(imgs_class, class_type, model, processor):\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text= ['A human-made photo of a' + str(class_type), 'A synthetic computer-generated photo of a' + str(class_type)],  # Prompts for each image\n",
    "        images=imgs_class,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob[:,1]\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images_class = {'airplane': [[],[]], 'automobile': [[],[]], 'bird': [[],[]], 'cat': [[],[]], 'deer': [[],[]], 'dog': [[],[]], 'frog': [[],[]], 'horse': [[],[]], 'ship': [[],[]], 'truck': [[],[]]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types.get(c)][0].append(img)\n",
    "                    images_class[class_types.get(c)][1].append(0)\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)\n",
    "\n",
    "    return images_class\n",
    "\n",
    "def evaluate_model(imgs_class):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cpu\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    labels = []\n",
    "    for i in imgs_class:\n",
    "        batch_probs = clip_pred(imgs_class[i][0], i,  model, processor)\n",
    "        probs.extend(batch_probs)\n",
    "        labels.extend(imgs_class[i][1])\n",
    "\n",
    "    probs = torch.tensor(probs)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(probs, labels)\n",
    "    f1.update(probs, labels)\n",
    "    cm.update(probs, labels)\n",
    "    auroc.update(probs, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "imgs_class = load_images_from_folder(folder_path)\n",
    "# Evaluate the model\n",
    "evaluate_model(imgs_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non HPC adjusted zero-shot with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "def clip_pred(imgs, imgs_class):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Pass the batch to the model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    auroc.update(ys, ts)\n",
    "\n",
    "    return acc.compute(), f1.compute(), cm.compute(), auroc.compute()\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images, imgs_class)\n",
    "metrics(probs[:, 1], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def clip_pred(imgs, imgs_class):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Pass the batch to the model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def evaluate_metrics(probs, labels):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "\n",
    "    acc.update(probs[:, 1], labels)\n",
    "    f1.update(probs[:, 1], labels)\n",
    "    cm.update(probs[:, 1], labels)\n",
    "    auroc.update(probs[:, 1], labels)\n",
    "\n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(labels.detach().numpy(), probs[:, 1].detach().numpy())\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, marker='.', label='ROC Curve (AUC = {:.2f})'.format(auroc_score))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images, imgs_class)\n",
    "evaluate_metrics(probs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted class code for HPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For running fake and real seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Check your GPU setup.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikto\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CLIPOutput' object has no attribute 'get_image_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m imgs_class\u001b[38;5;241m=\u001b[39m load_images_from_folder(folder_path)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_class\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 70\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(imgs_class)\u001b[0m\n\u001b[0;32m     68\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m imgs_class:\n\u001b[1;32m---> 70\u001b[0m     batch_probs \u001b[38;5;241m=\u001b[39m \u001b[43mclip_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_class\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     probs\u001b[38;5;241m.\u001b[39mextend(batch_probs)\n\u001b[0;32m     72\u001b[0m     labels\u001b[38;5;241m.\u001b[39mextend(imgs_class[i][\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[52], line 33\u001b[0m, in \u001b[0;36mclip_pred\u001b[1;34m(imgs_class, class_type, model, processor)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculation for inference\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m---> 33\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m], hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m logits_per_image \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits_per_image  \u001b[38;5;66;03m# Image-text similarity score\u001b[39;00m\n\u001b[0;32m     35\u001b[0m prob \u001b[38;5;241m=\u001b[39m logits_per_image\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Probability over classes\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CLIPOutput' object has no attribute 'get_image_features'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs_class, class_type, model, processor):\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text= ['A human-made photo of a' + str(class_type), 'A synthetic computer-generated photo of a' + str(class_type)],  # Prompts for each image\n",
    "        images=imgs_class,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.get_image_features(outputs['pixel_values'], hidden_states=False)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob[:,1], image_features\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images_class = {'airplane': [[],[]], 'automobile': [[],[]], 'bird': [[],[]], 'cat': [[],[]], 'deer': [[],[]], 'dog': [[],[]], 'frog': [[],[]], 'horse': [[],[]], 'ship': [[],[]], 'truck': [[],[]]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types.get(c)][0].append(img)\n",
    "                    images_class[class_types.get(c)][1].append(0)\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)\n",
    "\n",
    "    return images_class\n",
    "\n",
    "def evaluate_model(imgs_class):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cpu\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    labels = []\n",
    "    for i in imgs_class:\n",
    "        batch_probs = clip_pred(imgs_class[i][0], i,  model, processor)\n",
    "        probs.extend(batch_probs)\n",
    "        labels.extend(imgs_class[i][1])\n",
    "\n",
    "    probs = torch.tensor(probs)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(probs, labels)\n",
    "    f1.update(probs, labels)\n",
    "    cm.update(probs, labels)\n",
    "    auroc.update(probs, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "\n",
    "\n",
    "# Paths to the datasets\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "\n",
    "# Load the images and labels\n",
    "imgs_class= load_images_from_folder(folder_path)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(imgs_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake and real at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, imgs_class, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    return images, torch.tensor(labels, dtype=torch.int), imgs_class\n",
    "\n",
    "def evaluate_model(images, labels, imgs_class, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_classes = imgs_class[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, batch_classes, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    auroc.update(preds, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels, imgs_class = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels, imgs_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting image features and applying FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikto\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.13333334028720856\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: \n",
      "tensor([[ 4., 26.],\n",
      "        [ 0.,  0.]])\n",
      "AUROC: 0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "def clip_pred(imgs_class, class_type, model, processor):\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text= ['A human-made photo of a' + str(class_type), 'A synthetic computer-generated photo of a' + str(class_type)],  # Prompts for each image\n",
    "        images=imgs_class,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        img_features = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob[:,1], img_features\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images_class = {'airplane': [[],[]], 'automobile': [[],[]], 'bird': [[],[]], 'cat': [[],[]], 'deer': [[],[]], 'dog': [[],[]], 'frog': [[],[]], 'horse': [[],[]], 'ship': [[],[]], 'truck': [[],[]]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types.get(c)][0].append(img)\n",
    "                    images_class[class_types.get(c)][1].append(0)\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)\n",
    "\n",
    "    return images_class\n",
    "\n",
    "def evaluate_model(imgs_class):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cpu\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    labels = []\n",
    "    image_features = []\n",
    "    for i in imgs_class:\n",
    "        batch_probs, batch_features = clip_pred(imgs_class[i][0], i,  model, processor)\n",
    "        probs.extend(batch_probs)\n",
    "        labels.extend(imgs_class[i][1])\n",
    "        image_features.extend(batch_features)#TODO: features gets appended as tensors inside tensors \n",
    "    \n",
    "    probs = torch.tensor(probs)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    dataset = {'features': image_features, 'labels': labels}\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(probs, labels)\n",
    "    f1.update(probs, labels)\n",
    "    cm.update(probs, labels)\n",
    "    auroc.update(probs, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "imgs_class = load_images_from_folder(folder_path)\n",
    "# Evaluate the model\n",
    "feature_data = evaluate_model(imgs_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed foward neural network on image features from encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class dict_to_data(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict_to_data(feature_data['features'], feature_data['labels'])\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.2830, -0.0989, -0.2522,  ...,  0.7873, -0.1310,  0.1398],\n",
       "         [ 0.2830, -0.0989, -0.2522,  ...,  0.7873, -0.1310,  0.1398],\n",
       "         [ 0.2830, -0.0989, -0.2522,  ...,  0.7873, -0.1310,  0.1398],\n",
       "         [ 0.3200,  0.0135, -0.2140,  ...,  0.9258,  0.1744,  0.2087],\n",
       "         [ 0.3729, -0.2227, -0.0361,  ...,  0.8791, -0.0896,  0.0022],\n",
       "         [ 0.4326,  0.1218, -0.0874,  ...,  0.8460, -0.0576, -0.0450]]),\n",
       " tensor([[ 0.3200,  0.0135, -0.2140,  ...,  0.9258,  0.1744,  0.2087],\n",
       "         [ 0.3200,  0.0135, -0.2140,  ...,  0.9258,  0.1744,  0.2087]]),\n",
       " tensor([[ 0.4326,  0.1218, -0.0874,  ...,  0.8460, -0.0576, -0.0450],\n",
       "         [ 0.4326,  0.1218, -0.0874,  ...,  0.8460, -0.0576, -0.0450]]),\n",
       " tensor([[ 0.3729, -0.2227, -0.0361,  ...,  0.8791, -0.0896,  0.0022],\n",
       "         [ 0.3729, -0.2227, -0.0361,  ...,  0.8791, -0.0896,  0.0022],\n",
       "         [ 0.1723, -0.2688, -0.3962,  ...,  0.7089, -0.0830,  0.0771],\n",
       "         [ 0.1723, -0.2688, -0.3962,  ...,  0.7089, -0.0830,  0.0771],\n",
       "         [ 0.3892, -0.2487, -0.0842,  ...,  0.7336,  0.0558,  0.1818],\n",
       "         [ 0.3892, -0.2487, -0.0842,  ...,  0.7336,  0.0558,  0.1818]]),\n",
       " tensor([[ 0.3200,  0.0135, -0.2140,  ...,  0.9258,  0.1744,  0.2087],\n",
       "         [ 0.1723, -0.2688, -0.3962,  ...,  0.7089, -0.0830,  0.0771],\n",
       "         [ 0.3892, -0.2487, -0.0842,  ...,  0.7336,  0.0558,  0.1818]]),\n",
       " tensor([[ 0.4326,  0.1218, -0.0874,  ...,  0.8460, -0.0576, -0.0450],\n",
       "         [ 0.1723, -0.2688, -0.3962,  ...,  0.7089, -0.0830,  0.0771],\n",
       "         [ 0.3892, -0.2487, -0.0842,  ...,  0.7336,  0.0558,  0.1818]]),\n",
       " tensor([[ 0.3729, -0.2227, -0.0361,  ...,  0.8791, -0.0896,  0.0022],\n",
       "         [ 0.1723, -0.2688, -0.3962,  ...,  0.7089, -0.0830,  0.0771],\n",
       "         [ 0.3892, -0.2487, -0.0842,  ...,  0.7336,  0.0558,  0.1818]]),\n",
       " tensor([[ 3.1996e-01,  1.3471e-02, -2.1397e-01, -3.9262e-01,  2.0837e-01,\n",
       "          -4.0408e-02,  1.7131e-01,  7.1624e-01,  4.5993e-04,  9.0427e-02,\n",
       "           5.8518e-01,  4.3407e-01,  4.2729e-01, -1.8623e-01,  1.2680e-02,\n",
       "          -1.1537e-01,  2.3834e-01,  3.6140e-01, -3.9418e-02,  2.7290e-01,\n",
       "           7.4444e-01,  2.5047e-02,  5.1005e-02, -1.3094e-01, -6.2146e-02,\n",
       "           6.7720e-02, -2.0646e-03, -1.5554e-01, -4.4195e-04, -3.3020e-02,\n",
       "           3.3990e-01, -2.2186e-01,  4.5959e-02,  9.2573e-02, -9.8983e-01,\n",
       "           7.2300e-02, -4.0345e-01, -5.1391e-02, -4.6953e-01,  6.6319e-01,\n",
       "          -1.7381e-01, -3.2553e-02, -1.2224e-01, -1.8982e-01,  5.9075e-02,\n",
       "           7.5912e-01, -1.4067e-02,  3.2922e-01,  2.2696e-01, -6.2971e-01,\n",
       "           1.2441e-01, -4.0928e-02,  1.9443e-01,  2.7444e-01,  2.4248e-01,\n",
       "          -3.3892e-01,  5.0162e-01,  5.2114e-02, -1.0687e-02, -5.7181e-01,\n",
       "          -5.0650e-01,  3.9061e-02,  4.6564e-02,  2.0087e-02,  6.5976e-02,\n",
       "           5.6380e-02,  5.5728e-02,  6.6748e-02, -2.0830e-01, -1.4938e-01,\n",
       "          -7.6517e-02, -8.6356e-02,  1.7253e-01, -5.5159e-02, -4.3408e-01,\n",
       "           1.2864e-01, -1.8362e-01,  2.2462e-02, -2.5324e-01, -1.3940e-01,\n",
       "           6.2355e-02, -9.6860e-02,  2.6017e-01, -3.3710e-01,  6.4439e-01,\n",
       "           2.9813e-01,  1.0492e+00, -8.1939e-02, -1.0927e-01, -2.7779e-01,\n",
       "          -1.1184e-01, -1.2621e-01, -8.6671e+00,  5.4199e-03,  2.6140e-01,\n",
       "           1.4105e-01, -4.5812e-01, -4.0342e-01,  1.9838e-01, -1.7358e+00,\n",
       "           6.6481e-02, -1.0587e-01, -7.8231e-02,  1.7408e-01,  1.3455e-01,\n",
       "           2.6150e-01, -2.1161e+00,  2.6525e-01, -2.3945e-01, -1.3099e-01,\n",
       "          -4.7364e-02, -3.1229e-01, -2.0319e-01,  1.5093e-01,  1.3885e-01,\n",
       "          -9.6399e-03,  1.6341e-01, -2.5209e-01,  2.1915e-01,  1.8718e-03,\n",
       "          -1.3285e-01, -7.0313e-01, -4.4954e-01, -7.1731e-02,  1.3540e-01,\n",
       "          -1.7452e-01, -7.0274e-02,  3.4053e-01,  9.1967e-02,  5.4242e-02,\n",
       "          -6.4360e-03, -4.2021e-01,  4.2274e-01,  1.1437e+00,  2.4372e-01,\n",
       "           3.3705e-01,  3.3096e-01, -3.7217e-01, -5.7764e-02,  5.0983e-01,\n",
       "          -3.0141e-03,  5.8513e-02, -2.8106e-01, -2.1615e-01,  2.1397e-01,\n",
       "           1.3514e-01, -4.9422e-01,  6.9839e-01, -9.7563e-02,  1.0466e-01,\n",
       "          -1.5164e-01, -4.2242e-01,  7.0784e-01,  4.3184e-01, -2.0934e-01,\n",
       "          -2.3886e-01,  1.0580e-01, -4.7460e-02,  1.1061e-01, -1.6850e-01,\n",
       "           2.0842e-01, -5.2775e-01,  2.1616e-01,  4.9370e-02, -1.6657e-01,\n",
       "           1.2499e-01, -6.8213e-01,  4.3820e-01,  1.0197e-01,  5.1128e-02,\n",
       "           5.3118e-01, -1.5022e-01,  4.9044e-02, -8.5847e-02,  3.8886e-01,\n",
       "          -3.0220e-01,  1.0874e+00,  2.3297e-02,  2.9508e-02,  1.2108e-02,\n",
       "           1.8615e-01, -1.7923e-01, -6.4421e-02,  3.6517e-01, -6.5812e-01,\n",
       "          -4.9828e-01, -1.7862e-01,  3.5597e-01, -1.1176e-01, -1.6201e-01,\n",
       "           4.0855e-02,  1.5478e-01, -4.8169e-01, -1.3349e-01, -2.8180e-01,\n",
       "           1.1521e-01, -2.5688e-01, -2.3872e-01, -3.2628e-01,  4.4094e-01,\n",
       "           2.8659e-01,  1.7294e-01,  9.7540e-02,  2.7613e-01,  1.0568e-01,\n",
       "           2.9891e-01, -1.5852e-02, -2.2961e-01,  4.7245e-01, -5.1764e-01,\n",
       "           3.5860e-01,  2.4898e-01, -8.4777e-02, -6.6528e-02, -1.5586e-01,\n",
       "           8.7095e-02, -2.0805e-01,  3.6254e-01, -3.1856e-01, -2.0501e-01,\n",
       "           6.6562e-01, -6.4600e-01,  3.1591e-01,  1.8253e-01, -1.3612e-02,\n",
       "           5.7453e-01,  1.6204e-01,  7.2928e-02,  1.7273e-02,  3.2849e-01,\n",
       "          -1.2998e-01,  1.3535e-01,  2.0840e-01,  8.8723e-01, -2.2271e-01,\n",
       "          -1.0320e-01,  6.7970e-03, -5.0191e-01, -1.1384e-01, -3.2192e-01,\n",
       "          -1.4369e-01,  2.4696e-01,  5.7731e-01, -5.3808e-02,  2.2006e-01,\n",
       "          -5.5185e-01, -3.4631e-01,  1.3069e-02,  7.1377e-02,  5.3162e-01,\n",
       "          -2.2620e-01,  3.0315e-01,  3.7757e-01,  1.3732e-01,  1.9355e-01,\n",
       "           4.1732e-01, -3.8837e-01, -2.9810e-01,  7.1204e-01, -7.4452e-01,\n",
       "          -1.5540e-01,  3.1881e-02, -7.2392e-02, -5.6969e-01,  2.8076e-01,\n",
       "          -3.4135e-01,  9.1791e-02,  1.2282e-01,  6.1568e-01, -2.5362e-01,\n",
       "           3.5874e-01,  1.7180e-01, -2.4296e-01,  3.9669e-02,  1.7200e-02,\n",
       "          -1.2804e-01,  5.2852e-01, -3.2202e-01, -3.7648e-01, -9.6170e-02,\n",
       "          -2.8294e-01,  3.2112e-01,  1.0578e-02, -3.0210e-02,  2.2327e-01,\n",
       "          -1.7864e-01,  9.3406e-01, -1.5433e-02, -2.1918e-01,  2.7825e-01,\n",
       "           4.6094e-02,  3.2521e-01, -1.3820e-01,  2.4840e-01, -1.7979e-01,\n",
       "           7.0340e-01, -1.5297e-01, -5.3412e-01, -3.3169e-01, -2.4920e-01,\n",
       "           1.7299e-01, -2.2592e-01,  4.1105e-01,  1.8619e-01, -1.2695e-01,\n",
       "          -6.2037e-01,  6.4643e-02, -2.0404e-01,  1.1703e-02,  1.9349e-01,\n",
       "          -6.6725e-02, -4.4005e-01,  1.1423e+00, -1.5098e-01,  1.2174e-01,\n",
       "           1.3097e-01,  3.6028e-01,  1.0512e-01,  3.7376e-01, -5.9758e-02,\n",
       "           5.4365e-01,  1.0559e+00, -2.4219e-01, -3.6217e-01,  2.8751e-01,\n",
       "           3.9952e-01, -8.8989e-02,  4.1483e-01,  2.2226e-02,  9.4171e-01,\n",
       "           2.4125e-01, -2.7072e-01,  2.7477e-01, -1.5805e-01,  5.3019e-01,\n",
       "           8.7010e-02, -3.6511e-01,  4.3672e-01,  6.6345e-01, -1.0886e-01,\n",
       "           2.3262e-01,  1.0075e-01,  4.9116e-02, -6.1807e-02, -2.5034e-01,\n",
       "           2.8166e-01,  2.4800e-01,  1.4351e-01, -6.8339e-03, -3.5633e-02,\n",
       "          -3.2895e-01, -4.2165e-02,  3.4157e-01,  4.0173e-02,  5.7315e-02,\n",
       "           2.1075e-01, -6.7299e-02,  5.0468e-01, -8.6882e-02, -2.0350e-01,\n",
       "          -4.0801e-01,  2.4976e-01,  4.4980e-01, -1.6937e-02, -3.8697e-01,\n",
       "           1.4527e-01, -6.8582e-02,  3.7526e-01,  3.5992e-01, -3.2109e-01,\n",
       "          -1.5017e-02,  6.0490e-02,  7.0782e-02,  1.1626e-01,  1.6787e-01,\n",
       "           1.0744e-01,  3.9898e-01, -1.6001e-02, -1.8072e-01,  1.1302e-01,\n",
       "          -1.9718e-02,  5.4111e-01, -5.1842e-05,  1.4824e-01,  2.9921e-01,\n",
       "           2.0232e-01, -7.5758e-03,  3.9303e-01, -6.9342e-01,  2.2022e-01,\n",
       "          -4.8034e-01, -1.6826e-01,  3.0294e-01,  3.1708e-03,  7.6092e-02,\n",
       "           4.5555e-01, -4.5101e-01, -3.8894e-01,  1.2261e-01,  3.9244e-01,\n",
       "          -7.0266e-02,  2.7707e-01,  5.0179e-03,  1.2980e-01, -1.0482e-01,\n",
       "           6.4251e-02,  1.6731e-02,  2.4667e-02,  5.3776e-01, -3.5208e-01,\n",
       "          -1.0115e-01, -9.7900e-02,  1.9880e-01,  3.9339e-01, -1.9875e-01,\n",
       "           1.9546e-01, -5.7265e-01, -3.6826e-01, -5.3822e-01, -2.2709e-01,\n",
       "           5.3074e-02, -1.6573e-01, -1.0507e-01, -2.1343e-01, -2.7711e-01,\n",
       "           3.3873e-01,  3.8295e-01, -6.5880e-01, -8.5607e-01,  9.1331e-02,\n",
       "          -2.0789e-01, -1.6802e-01, -5.7536e-01, -5.7857e-01, -3.1844e-01,\n",
       "          -8.7815e-02,  7.7137e-02,  4.0169e-01, -6.3512e-02,  2.1861e-01,\n",
       "          -2.5818e-01,  2.0246e-01,  3.0907e-01,  5.5259e-02, -1.1169e-01,\n",
       "           3.0543e-01,  7.0988e-02,  2.2752e-01,  1.7327e-02,  3.9794e-02,\n",
       "          -5.8769e-01, -2.3837e-01, -1.7999e-01, -5.0487e-02, -6.4762e-02,\n",
       "          -3.6627e-01,  2.8420e-01, -2.0678e-01, -3.0787e-01, -2.3987e-01,\n",
       "           3.6324e-01,  8.7591e-03,  1.8736e-01, -2.5402e-01,  6.7424e-01,\n",
       "          -1.4020e-02,  6.9819e-01, -7.3623e-02, -1.3476e-02, -5.6541e-02,\n",
       "           2.2977e-01,  1.1386e-01, -2.4330e-01, -5.3200e-01,  1.9868e-01,\n",
       "          -5.2633e-01,  1.1885e-01, -4.1464e-01, -2.5700e-01, -4.3835e-01,\n",
       "           1.9601e-01, -4.1638e-02,  1.0930e-02,  3.8701e-01,  1.9250e-01,\n",
       "          -2.2617e-01, -2.2597e-01,  1.7474e-02, -8.8343e-02, -4.1890e-02,\n",
       "           1.7746e-01, -6.9740e-03, -1.4678e-01, -2.7765e-01,  6.2594e-02,\n",
       "          -2.5259e-01,  3.8141e-01, -3.7604e-01, -6.5070e-02, -3.3926e-01,\n",
       "           1.0290e-01, -8.2954e-02, -2.6449e-01,  2.1403e-01,  5.2207e-01,\n",
       "           2.6016e-01,  3.6694e-01,  1.3272e-01, -2.8618e-01,  9.2582e-01,\n",
       "           1.7438e-01,  2.0870e-01]]),\n",
       " tensor([[ 4.3257e-01,  1.2178e-01, -8.7422e-02, -6.2936e-03, -4.8841e-02,\n",
       "           2.5053e-02,  1.3953e-01,  6.1492e-01, -5.5485e-02,  3.8213e-01,\n",
       "           6.4786e-01,  3.2276e-01,  1.8555e-01, -1.7026e-01, -2.7127e-01,\n",
       "           2.9077e-01, -2.0402e-01,  1.7174e-01,  2.6147e-01, -1.2357e-01,\n",
       "          -2.3266e-01, -8.7943e-02, -1.1599e-01, -4.4458e-01, -3.3345e-01,\n",
       "           2.3701e-01,  3.6794e-01, -5.7258e-01,  1.0775e-01,  9.0151e-02,\n",
       "           5.2472e-01, -1.1743e-02,  1.9124e-01,  2.1328e-01, -6.4468e-01,\n",
       "          -1.4495e-01, -1.9180e-02,  8.0732e-02, -3.5859e-01,  2.3974e-01,\n",
       "           5.4939e-01, -5.2832e-02, -9.1539e-02, -1.6186e-01,  2.1672e-01,\n",
       "          -3.0587e-01,  5.2225e-02, -6.6230e-02,  1.0292e-01, -3.7855e-02,\n",
       "          -2.8512e-01,  2.2597e-01,  1.3218e-01, -2.1951e-02,  2.6361e-01,\n",
       "           2.1107e-01,  1.2539e-01,  1.3223e-01,  2.6783e-01,  5.3968e-02,\n",
       "           1.7503e-01,  1.2006e-01,  2.1455e-01, -4.0395e-01, -1.2022e-01,\n",
       "          -2.6651e-01, -2.5408e-01, -2.9758e-01, -1.9315e-01, -2.3010e-01,\n",
       "          -1.4799e-01,  8.8029e-02, -1.8878e-01,  4.8199e-01, -4.1733e-01,\n",
       "          -1.8516e-01,  2.7214e-01, -1.3492e-02, -4.2953e-01, -5.0145e-01,\n",
       "          -7.8585e-02, -2.0478e-01,  5.4961e-01, -4.4688e-01,  4.8397e-01,\n",
       "           2.3312e-01,  5.1208e-01, -3.1682e-01, -4.3983e-01, -6.7028e-01,\n",
       "           1.4243e-01,  5.4805e-02, -9.1347e+00,  2.1788e-01,  7.3332e-01,\n",
       "           2.7408e-01, -3.4913e-01, -4.7070e-02, -9.4752e-01, -1.7809e+00,\n",
       "          -1.2904e-01, -2.8802e-01,  2.8265e-01, -8.8659e-02,  2.9965e-01,\n",
       "           3.0998e-03, -1.8365e+00,  4.3434e-01, -3.2616e-01, -2.5059e-01,\n",
       "           1.3965e-01, -7.3216e-01,  3.8694e-02,  1.4809e-02,  2.5221e-01,\n",
       "           9.0025e-02, -5.7095e-01, -2.0143e-01,  5.0174e-01, -4.3583e-01,\n",
       "          -4.8531e-01, -2.8912e-01,  4.1561e-02, -1.2570e-01,  3.1307e-01,\n",
       "           6.8956e-02,  8.0072e-02, -1.2530e-03, -7.4145e-02,  2.0715e-01,\n",
       "           2.6898e-01, -1.6934e-02,  3.7608e-02,  1.1572e+00,  2.6347e-01,\n",
       "           3.3050e-01,  2.3370e-01, -5.5075e-01,  1.2542e-01,  4.0449e-01,\n",
       "           2.9366e-01, -5.8131e-02, -1.1578e-01, -1.2293e-01,  2.5091e-02,\n",
       "           4.7633e-01, -1.3633e-01,  5.8424e-01, -1.4823e-01,  8.5396e-02,\n",
       "           1.3471e-01,  1.3158e-01,  7.7431e-01, -2.1938e-01, -1.4288e-01,\n",
       "           1.5136e-01, -4.0877e-01,  1.8773e-01, -3.7569e-01, -4.3050e-01,\n",
       "          -1.0026e-01, -2.9367e-01,  5.6118e-01, -2.9139e-01,  1.3851e-01,\n",
       "           4.6209e-01, -1.9186e-01,  5.2482e-01,  2.5853e-01,  1.6309e-01,\n",
       "           2.4201e-01, -4.4002e-01,  2.4018e-01, -3.5314e-01,  1.2342e-01,\n",
       "          -2.3670e-01,  6.6892e-02,  1.9372e-01, -5.0590e-01, -3.2754e-01,\n",
       "          -2.3403e-01, -3.8010e-01, -5.6662e-02,  2.1312e-01, -4.3114e-01,\n",
       "          -4.6339e-01,  1.0041e-01,  1.2735e-01, -9.2848e-04, -7.8569e-02,\n",
       "          -4.2320e-01,  9.0711e-02, -4.7283e-01,  1.2018e-01,  3.6025e-01,\n",
       "          -1.1600e-01, -8.0510e-02, -1.1206e-01,  3.8328e-01,  1.2908e-01,\n",
       "           1.3458e-01,  1.4061e-01,  2.1096e-01,  2.2364e-01,  3.5823e-01,\n",
       "           1.6510e-01, -2.9341e-01, -2.2844e-01,  1.5878e-01, -8.0673e-02,\n",
       "          -3.3961e-02,  3.0917e-01, -2.3897e-01,  2.4056e-01, -3.1763e-01,\n",
       "           3.8044e-02, -4.6903e-01,  4.1829e-01,  6.3937e-01, -3.4802e-01,\n",
       "           4.7827e-01, -1.3119e-01,  4.0605e-01,  4.0510e-01,  3.5697e-01,\n",
       "           5.3768e-01,  1.6965e-01, -2.5878e-02, -2.8400e-01,  4.1916e-01,\n",
       "          -1.5447e-01,  5.4691e-02,  4.1461e-01,  2.1634e-01,  1.5648e-03,\n",
       "          -2.6199e-02,  2.9875e-03, -4.6467e-01, -3.1853e-01, -1.5407e-01,\n",
       "          -4.7253e-01, -1.8020e-01, -4.7648e-01, -3.0613e-01,  2.5586e-01,\n",
       "          -3.2501e-01, -6.0200e-01, -3.8299e-01,  1.0538e-01,  5.9642e-01,\n",
       "          -3.2014e-01,  4.7959e-01,  2.7034e-01, -1.2322e-01, -1.1772e-01,\n",
       "          -1.5274e-02, -1.7905e-01, -4.4803e-01,  4.4208e-01, -4.0722e-01,\n",
       "           2.2827e-02, -9.5923e-02, -4.3045e-01,  3.7185e-01,  2.7150e-01,\n",
       "           1.7364e-01,  1.3833e-01, -3.8629e-01,  4.5714e-01,  7.7557e-02,\n",
       "           6.9939e-01, -6.1764e-02,  6.0875e-02, -4.5708e-01, -3.0443e-01,\n",
       "           3.8913e-01,  5.0670e-01, -4.1065e-01,  1.2492e-01,  1.8888e-01,\n",
       "          -1.7499e-01,  4.0006e-01, -1.2961e-01,  5.5683e-02,  2.5849e-01,\n",
       "          -1.5014e-01, -5.7841e-01,  7.0052e-02, -8.9937e-01, -1.6587e-01,\n",
       "          -3.1619e-02,  3.2040e-01, -4.2951e-01,  1.5086e-01, -1.7273e-01,\n",
       "           2.3808e-01, -1.7274e-01,  3.2706e-02, -2.8495e-01, -1.2916e-01,\n",
       "          -1.2795e-01, -3.5435e-01,  3.5450e-01,  5.3794e-01,  3.1187e-03,\n",
       "          -1.4852e-01,  7.3146e-02,  1.3457e-01,  4.1994e-01,  2.2901e-01,\n",
       "          -4.2127e-01,  8.1975e-02,  1.1572e+00, -4.6606e-01,  1.9942e-01,\n",
       "           1.0156e-01,  1.8760e-01,  4.2917e-01, -1.3975e-01, -3.8390e-01,\n",
       "           5.6721e-01,  1.2298e+00,  1.8923e-01, -2.0121e-01,  1.5962e-01,\n",
       "           1.9765e-01, -2.6131e-01,  2.8287e-01, -2.4342e-01,  8.0799e-01,\n",
       "          -3.3252e-01, -3.0249e-01,  5.9120e-02, -1.1372e-01,  2.9090e-01,\n",
       "           2.5198e-01, -1.5691e-01, -1.0155e-01,  5.5567e-01, -2.9462e-02,\n",
       "           2.9218e-01, -6.5897e-02, -3.8229e-01,  1.5438e-01, -4.1441e-01,\n",
       "          -6.2217e-03,  8.8114e-02, -2.2043e-01,  1.1957e-01,  5.3019e-01,\n",
       "           1.2864e-01, -5.5468e-01,  3.6032e-01, -1.9198e-01, -6.1919e-02,\n",
       "          -2.0722e-01, -2.3584e-01, -4.1264e-01, -2.4724e-01,  3.0927e-02,\n",
       "           6.5743e-02,  3.6658e-01,  1.4075e-01,  2.8902e-02, -9.4411e-01,\n",
       "           6.1415e-02,  4.4345e-01,  3.3745e-01, -4.5256e-01, -3.2634e-02,\n",
       "           7.6224e-02,  2.6917e-01,  1.0296e-01, -9.8498e-02,  2.8811e-01,\n",
       "           4.9377e-01,  5.7034e-01,  1.3500e-01,  1.8689e-01, -6.7467e-02,\n",
       "          -2.6250e-01, -8.2916e-02,  2.3741e-03, -3.3285e-01, -2.1984e-01,\n",
       "           3.7700e-01,  4.0427e-02,  3.7400e-01, -3.1105e-01,  3.1950e-01,\n",
       "          -3.4626e-01, -5.3966e-01,  1.9778e-01,  8.4056e-02,  3.8601e-02,\n",
       "           3.5838e-01, -2.2677e-01, -1.5789e-01, -4.0513e-01,  3.7559e-02,\n",
       "          -3.8522e-01,  1.4062e-01,  2.1318e-01, -2.7156e-01,  1.4918e-01,\n",
       "          -3.5086e-01,  2.2144e-02,  1.8142e-01,  5.8832e-01, -1.8486e-01,\n",
       "           6.1498e-01, -9.3984e-02, -2.7569e-02,  4.1062e-02, -9.5653e-02,\n",
       "           2.6415e-01, -3.5822e-01, -5.5108e-01, -3.3995e-01, -9.7460e-02,\n",
       "          -2.5946e-01, -2.5582e-01, -4.1797e-01, -4.8278e-01, -3.0616e-01,\n",
       "          -3.9508e-02,  5.3014e-01, -4.4387e-01, -9.2306e-01,  1.3536e-01,\n",
       "           5.1741e-01,  1.2518e-01, -2.9709e-01, -4.1457e-01, -2.7366e-02,\n",
       "          -6.5836e-02,  1.0736e-01,  3.3587e-01,  2.8828e-01, -3.3878e-01,\n",
       "          -2.4836e-01,  2.6721e-01,  2.6270e-01, -4.7507e-02, -1.2183e-01,\n",
       "           3.7307e-01, -6.7790e-02,  3.6732e-01, -2.8251e-01, -1.8303e-01,\n",
       "          -3.8197e-01, -3.6364e-01, -1.4106e-01, -2.5246e-01, -1.0620e-01,\n",
       "           1.2512e-01, -6.9646e-02, -2.0674e-01, -6.0376e-02, -9.3742e-02,\n",
       "           3.4939e-01,  3.1818e-01,  2.1286e-01, -2.3730e-02,  2.4042e-01,\n",
       "          -2.6132e-01,  3.7464e-01, -1.3605e-01, -2.3621e-02,  1.3415e-01,\n",
       "           1.7666e-01,  4.6439e-01,  2.7039e-01, -2.9443e-01,  6.6873e-02,\n",
       "          -6.9380e-01, -2.0000e-01, -2.4757e-01,  3.3549e-02, -3.5126e-01,\n",
       "           8.1468e-02,  3.8940e-01, -3.6460e-01,  4.7730e-01,  2.6448e-01,\n",
       "           3.6706e-01,  2.5724e-01,  2.1962e-01, -1.0183e-01, -1.7946e-01,\n",
       "          -1.5472e-02, -8.0843e-04,  4.7767e-01, -6.8662e-01, -7.1702e-02,\n",
       "          -2.7997e-01,  4.9410e-01,  1.3853e-01, -7.6978e-02, -3.8205e-01,\n",
       "          -9.1393e-03,  5.0632e-02,  7.4158e-02,  7.2653e-02,  7.3293e-01,\n",
       "           8.2219e-02, -1.8424e-02,  2.7355e-01, -1.4803e-01,  8.4597e-01,\n",
       "          -5.7565e-02, -4.4974e-02]]),\n",
       " tensor([[ 0.2830, -0.0989, -0.2522,  ...,  0.7873, -0.1310,  0.1398],\n",
       "         [ 0.2830, -0.0989, -0.2522,  ...,  0.7873, -0.1310,  0.1398],\n",
       "         [ 0.3729, -0.2227, -0.0361,  ...,  0.8791, -0.0896,  0.0022]])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      5\u001b[0m num_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mnetwork\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_features = dataset.features.shape[1]\n",
    "num_hidden = 256\n",
    "\n",
    "class network(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_hidden):\n",
    "\n",
    "        super(network, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "ffnn = network(num_features, num_hidden)\n",
    "ffnn.to(\"cpu\")\n",
    "print(ffnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(ffnn.parameters(), lr=0.0001)\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6832132935523987, Accuracy: 50.0\n",
      "Epoch 2, Loss: 0.6568736433982849, Accuracy: 50.0\n",
      "Epoch 3, Loss: 0.631369948387146, Accuracy: 60.0\n",
      "Epoch 4, Loss: 0.6067826151847839, Accuracy: 60.0\n",
      "Epoch 5, Loss: 0.5831183791160583, Accuracy: 60.0\n",
      "Epoch 6, Loss: 0.5603747963905334, Accuracy: 60.0\n",
      "Epoch 7, Loss: 0.5384977459907532, Accuracy: 60.0\n",
      "Epoch 8, Loss: 0.5174135565757751, Accuracy: 60.0\n",
      "Epoch 9, Loss: 0.49717918038368225, Accuracy: 60.0\n",
      "Epoch 10, Loss: 0.47783994674682617, Accuracy: 60.0\n",
      "Epoch 11, Loss: 0.45935192704200745, Accuracy: 60.0\n",
      "Epoch 12, Loss: 0.4416030943393707, Accuracy: 60.0\n",
      "Epoch 13, Loss: 0.4247424006462097, Accuracy: 60.0\n",
      "Epoch 14, Loss: 0.4088355004787445, Accuracy: 60.0\n",
      "Epoch 15, Loss: 0.393624871969223, Accuracy: 60.0\n",
      "Epoch 16, Loss: 0.37901490926742554, Accuracy: 60.0\n",
      "Epoch 17, Loss: 0.36505797505378723, Accuracy: 60.0\n",
      "Epoch 18, Loss: 0.3517361283302307, Accuracy: 60.0\n",
      "Epoch 19, Loss: 0.3388766646385193, Accuracy: 60.0\n",
      "Epoch 20, Loss: 0.3265123963356018, Accuracy: 60.0\n",
      "Epoch 21, Loss: 0.3146651089191437, Accuracy: 60.0\n",
      "Epoch 22, Loss: 0.30328088998794556, Accuracy: 60.0\n",
      "Epoch 23, Loss: 0.2923387289047241, Accuracy: 60.0\n",
      "Epoch 24, Loss: 0.2818291485309601, Accuracy: 60.0\n",
      "Epoch 25, Loss: 0.2717455327510834, Accuracy: 60.0\n",
      "Epoch 26, Loss: 0.26207610964775085, Accuracy: 60.0\n",
      "Epoch 27, Loss: 0.2528384327888489, Accuracy: 60.0\n",
      "Epoch 28, Loss: 0.24397993087768555, Accuracy: 60.0\n",
      "Epoch 29, Loss: 0.23546555638313293, Accuracy: 60.0\n",
      "Epoch 30, Loss: 0.22728125751018524, Accuracy: 60.0\n",
      "Epoch 31, Loss: 0.21943417191505432, Accuracy: 60.0\n",
      "Epoch 32, Loss: 0.2119019329547882, Accuracy: 60.0\n",
      "Epoch 33, Loss: 0.20466166734695435, Accuracy: 60.0\n",
      "Epoch 34, Loss: 0.1977231651544571, Accuracy: 60.0\n",
      "Epoch 35, Loss: 0.19107817113399506, Accuracy: 60.0\n",
      "Epoch 36, Loss: 0.1847115457057953, Accuracy: 60.0\n",
      "Epoch 37, Loss: 0.17859745025634766, Accuracy: 60.0\n",
      "Epoch 38, Loss: 0.17270639538764954, Accuracy: 60.0\n",
      "Epoch 39, Loss: 0.16704629361629486, Accuracy: 60.0\n",
      "Epoch 40, Loss: 0.1616305708885193, Accuracy: 60.0\n",
      "Epoch 41, Loss: 0.15641771256923676, Accuracy: 60.0\n",
      "Epoch 42, Loss: 0.1514073759317398, Accuracy: 60.0\n",
      "Epoch 43, Loss: 0.14659476280212402, Accuracy: 60.0\n",
      "Epoch 44, Loss: 0.14197923243045807, Accuracy: 60.0\n",
      "Epoch 45, Loss: 0.13755275309085846, Accuracy: 60.0\n",
      "Epoch 46, Loss: 0.13330577313899994, Accuracy: 60.0\n",
      "Epoch 47, Loss: 0.12923724949359894, Accuracy: 60.0\n",
      "Epoch 48, Loss: 0.12532417476177216, Accuracy: 60.0\n",
      "Epoch 49, Loss: 0.12156615406274796, Accuracy: 60.0\n",
      "Epoch 50, Loss: 0.11795751750469208, Accuracy: 60.0\n",
      "Epoch 51, Loss: 0.11448898911476135, Accuracy: 60.0\n",
      "Epoch 52, Loss: 0.11114717274904251, Accuracy: 60.0\n",
      "Epoch 53, Loss: 0.10793912410736084, Accuracy: 60.0\n",
      "Epoch 54, Loss: 0.10485931485891342, Accuracy: 60.0\n",
      "Epoch 55, Loss: 0.1018953025341034, Accuracy: 60.0\n",
      "Epoch 56, Loss: 0.0990440770983696, Accuracy: 60.0\n",
      "Epoch 57, Loss: 0.09630396962165833, Accuracy: 60.0\n",
      "Epoch 58, Loss: 0.0936674103140831, Accuracy: 60.0\n",
      "Epoch 59, Loss: 0.0911283940076828, Accuracy: 60.0\n",
      "Epoch 60, Loss: 0.0886828675866127, Accuracy: 60.0\n",
      "Epoch 61, Loss: 0.08633026480674744, Accuracy: 60.0\n",
      "Epoch 62, Loss: 0.08406683802604675, Accuracy: 60.0\n",
      "Epoch 63, Loss: 0.0818861797451973, Accuracy: 60.0\n",
      "Epoch 64, Loss: 0.07978599518537521, Accuracy: 60.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 64\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ffnn.train()\n",
    "    for batch_features, batch_labels in train_loader:  \n",
    "        batch_features, batch_labels = batch_features.to(\"cpu\"), batch_labels.to(\"cpu\")\n",
    "\n",
    "        batch_output = ffnn(batch_features)\n",
    "        predictions = (batch_output > 0.5).float()\n",
    "        batch_loss = loss(batch_output.squeeze(), batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_accuracy = torch.sum(predictions == batch_labels)/len(predictions)\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {batch_loss.item()}, Accuracy: {batch_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
