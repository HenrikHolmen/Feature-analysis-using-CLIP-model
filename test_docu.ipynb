{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot without classes incoporated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryPrecisionRecallCurve\n",
    "\n",
    "def clip_pred(imgs):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "        \n",
    "    \n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\DTU_repo\\deep_learning\\4_Convolutional\\images'\n",
    "images, labels = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images)\n",
    "metrics(probs[:,1], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC adjusted zero-shot no classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    \"\"\"\n",
    "    Load images and their corresponding labels from specified folders.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "    \n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "    \n",
    "    return images, torch.tensor(labels, dtype=torch.int)\n",
    "\n",
    "def evaluate_model(images, labels, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    \n",
    "    print(f\"Accuracy: {acc.compute().item():.4f}\")\n",
    "    print(f\"F1 Score: {f1.compute().item():.4f}\")\n",
    "    print(f\"Confusion Matrix: {cm.compute()}\")\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot with classes implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "def clip_pred(imgs_class, class_type, model, processor):\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text= ['A human-made photo of a' + str(class_type), 'A synthetic computer-generated photo of a' + str(class_type)],  # Prompts for each image\n",
    "        images=imgs_class,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob[:,1]\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images_class = {'airplane': [[],[]], 'automobile': [[],[]], 'bird': [[],[]], 'cat': [[],[]], 'deer': [[],[]], 'dog': [[],[]], 'frog': [[],[]], 'horse': [[],[]], 'ship': [[],[]], 'truck': [[],[]]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types.get(c)][0].append(img)\n",
    "                    images_class[class_types.get(c)][1].append(0)\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)\n",
    "\n",
    "    return images_class\n",
    "\n",
    "def evaluate_model(imgs_class):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cpu\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    labels = []\n",
    "    for i in imgs_class:\n",
    "        batch_probs = clip_pred(imgs_class[i][0], i,  model, processor)\n",
    "        probs.extend(batch_probs)\n",
    "        labels.extend(imgs_class[i][1])\n",
    "\n",
    "    probs = torch.tensor(probs)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(probs, labels)\n",
    "    f1.update(probs, labels)\n",
    "    cm.update(probs, labels)\n",
    "    auroc.update(probs, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "imgs_class = load_images_from_folder(folder_path)\n",
    "# Evaluate the model\n",
    "evaluate_model(imgs_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non HPC adjusted zero-shot with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "def clip_pred(imgs, imgs_class):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Pass the batch to the model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    auroc.update(ys, ts)\n",
    "\n",
    "    return acc.compute(), f1.compute(), cm.compute(), auroc.compute()\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images, imgs_class)\n",
    "metrics(probs[:, 1], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def clip_pred(imgs, imgs_class):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Pass the batch to the model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def evaluate_metrics(probs, labels):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "\n",
    "    acc.update(probs[:, 1], labels)\n",
    "    f1.update(probs[:, 1], labels)\n",
    "    cm.update(probs[:, 1], labels)\n",
    "    auroc.update(probs[:, 1], labels)\n",
    "\n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(labels.detach().numpy(), probs[:, 1].detach().numpy())\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, marker='.', label='ROC Curve (AUC = {:.2f})'.format(auroc_score))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images, imgs_class)\n",
    "evaluate_metrics(probs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted class code for HPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For running fake and real seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs_class, class_type, model, processor):\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text= ['A human-made photo of a' + str(class_type), 'A synthetic computer-generated photo of a' + str(class_type)],  # Prompts for each image\n",
    "        images=imgs_class,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.get_image_features(outputs['pixel_values'], hidden_states=False)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob[:,1], image_features\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images_class = {'airplane': [[],[]], 'automobile': [[],[]], 'bird': [[],[]], 'cat': [[],[]], 'deer': [[],[]], 'dog': [[],[]], 'frog': [[],[]], 'horse': [[],[]], 'ship': [[],[]], 'truck': [[],[]]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types.get(c)][0].append(img)\n",
    "                    images_class[class_types.get(c)][1].append(0)\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)\n",
    "\n",
    "    return images_class\n",
    "\n",
    "def evaluate_model(imgs_class):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cpu\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    labels = []\n",
    "    for i in imgs_class:\n",
    "        batch_probs = clip_pred(imgs_class[i][0], i,  model, processor)\n",
    "        probs.extend(batch_probs)\n",
    "        labels.extend(imgs_class[i][1])\n",
    "\n",
    "    probs = torch.tensor(probs)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(probs, labels)\n",
    "    f1.update(probs, labels)\n",
    "    cm.update(probs, labels)\n",
    "    auroc.update(probs, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "\n",
    "\n",
    "# Paths to the datasets\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "\n",
    "# Load the images and labels\n",
    "imgs_class= load_images_from_folder(folder_path)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(imgs_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake and real at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, imgs_class, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    return images, torch.tensor(labels, dtype=torch.int), imgs_class\n",
    "\n",
    "def evaluate_model(images, labels, imgs_class, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_classes = imgs_class[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, batch_classes, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    auroc.update(preds, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels, imgs_class = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels, imgs_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "l = [[1,2,3], [4,5,6], [7,8,9]]\n",
    "j=[]\n",
    "for i in l:\n",
    "    j.extend(i)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting image features and applying FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikto\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.13333334028720856\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: \n",
      "tensor([[ 4., 26.],\n",
      "        [ 0.,  0.]])\n",
      "AUROC: 0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clip_pred(imgs_class, class_type, model, processor):\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text= ['A human-made photo of a' + str(class_type), 'A synthetic computer-generated photo of a' + str(class_type)],  # Prompts for each image\n",
    "        images=imgs_class,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        img_features = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    pred = (prob[:,1] > 0.5).int()\n",
    "    return pred, img_features\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images_class = {'airplane': [[],[]], 'automobile': [[],[]], 'bird': [[],[]], 'cat': [[],[]], 'deer': [[],[]], 'dog': [[],[]], 'frog': [[],[]], 'horse': [[],[]], 'ship': [[],[]], 'truck': [[],[]]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types.get(c)][0].append(img)\n",
    "                    images_class[class_types.get(c)][1].append(0)\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)\n",
    "    \n",
    "\n",
    "    return images_class\n",
    "\n",
    "def evaluate_model(imgs_class):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cpu\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    labels = []\n",
    "    image_features = []\n",
    "    for i in imgs_class:\n",
    "        batch_probs, batch_features = clip_pred(imgs_class[i][0], i,  model, processor)\n",
    "        probs.extend(batch_probs)\n",
    "        labels.extend(imgs_class[i][1])\n",
    "        for f in batch_features:\n",
    "            image_features.append(f)\n",
    "    \n",
    "    probs = torch.tensor(probs)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    dataset = {'features': image_features, 'labels': labels}\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(probs, labels)\n",
    "    f1.update(probs, labels)\n",
    "    cm.update(probs, labels)\n",
    "    auroc.update(probs, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "imgs_class = load_images_from_folder(folder_path)\n",
    "# Evaluate the model\n",
    "feature_data = evaluate_model(imgs_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed foward neural network on image features from encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class dict_to_data(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict_to_data(feature_data['features'], feature_data['labels'])\n",
    "train_loader = DataLoader(dataset, batch_size=10, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_features = len(dataset.features[0])\n",
    "num_hidden = 256\n",
    "\n",
    "class network(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_hidden):\n",
    "\n",
    "        super(network, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "ffnn = network(num_features, num_hidden)\n",
    "ffnn.to(\"cpu\")\n",
    "print(ffnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(ffnn.parameters(), lr=0.0001)\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:00<00:00, 428.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6302995085716248, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:00<00:00, 544.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.6309584379196167, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:00<00:00, 749.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6296683549880981, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:00<00:00, 499.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.6382250785827637, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:00<00:00, 750.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.634983479976654, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:00<00:00, 749.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.633604884147644, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:00<00:00, 750.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.6356449723243713, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:00<00:00, 599.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.6323121190071106, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:00<00:00, 749.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.6389063596725464, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:00<00:00, 750.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.6283644437789917, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_loop(train_loader, model, loss, num_epochs):\n",
    "     \n",
    "    model.train()\n",
    "    acc = BinaryAccuracy()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_features, batch_labels in tqdm(train_loader, desc = \"Training\"):  \n",
    "            batch_features, batch_labels = batch_features.to(\"cpu\"), batch_labels.to(\"cpu\")\n",
    "\n",
    "            batch_output = model(batch_features).squeeze()\n",
    "            batch_preds = (batch_output > 0.5).int()\n",
    "            batch_loss = loss(batch_output, batch_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc.update(batch_preds, batch_labels)\n",
    "            batch_accuracy = acc.compute()\n",
    "            acc.reset()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {batch_loss}, Accuracy: {batch_accuracy}\")\n",
    "train_loop(train_loader, ffnn, nn.BCELoss(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 750.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6310, Test Accuracy: 1.0000\n",
      "Test Loss: 0.6356, Test Accuracy: 1.0000\n",
      "Test Loss: 0.6270, Test Accuracy: 1.0000\n",
      "Average test loss: 0.6312, Average test accuracy: 1.0000, Confusion Matrix: tensor([[30.,  0.],\n",
      "        [ 0.,  0.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_network(test_loader, model, loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    acc = BinaryAccuracy()\n",
    "    test_cm = BinaryConfusionMatrix()\n",
    "    test_auroc = BinaryAUROC()\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in tqdm(test_loader, desc = \"Testing\"):\n",
    "            batch_features, batch_labels = batch_features.to('cpu'), batch_labels.to('cpu')\n",
    "\n",
    "            # Forward pass\n",
    "            batch_outputs = model(batch_features).squeeze()\n",
    "            batch_loss = loss(batch_outputs, batch_labels)\n",
    "            test_loss += batch_loss\n",
    "            batch_labels = batch_labels.to(torch.int64)\n",
    "\n",
    "            acc.update(batch_outputs, batch_labels)\n",
    "            test_cm.update(batch_outputs, batch_labels)\n",
    "            batch_accuracy = acc.compute()\n",
    "            test_acc += batch_accuracy\n",
    "            acc.reset()\n",
    "            print(f'Test Loss: {batch_loss:.4f}, Test Accuracy: {batch_accuracy:.4f}')\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    avg_acc =  test_acc / len(test_loader)\n",
    "    cm = test_cm.compute()\n",
    "\n",
    "    print(f'Average test loss: {avg_loss:.4f}, Average test accuracy: {avg_acc:.4f}, Confusion Matrix: {cm}')\n",
    "\n",
    "test_network(train_loader ,model = ffnn, loss = nn.BCELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class airplane (1/10)\n",
      "Processing class automobile (2/10)\n",
      "Processing class bird (3/10)\n",
      "Processing class cat (4/10)\n",
      "Processing class deer (5/10)\n",
      "Processing class dog (6/10)\n",
      "Processing class frog (7/10)\n",
      "Processing class horse (8/10)\n",
      "Processing class ship (9/10)\n",
      "Processing class truck (10/10)\n",
      "Accuracy: 0.5000\n",
      "F1 Score: 0.6667\n",
      "Confusion Matrix: \n",
      "tensor([[ 0., 30.],\n",
      "        [ 0., 30.]])\n",
      "AUROC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/4: 100%|██████████| 30/30 [00:00<00:00, 540.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7100, Accuracy: 0.4490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/4: 100%|██████████| 30/30 [00:00<00:00, 548.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.6987, Accuracy: 0.5014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/4: 100%|██████████| 30/30 [00:00<00:00, 578.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6975, Accuracy: 0.3872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/4: 100%|██████████| 30/30 [00:00<00:00, 572.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.6990, Accuracy: 0.4836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class airplane (1/10)\n",
      "Processing class automobile (2/10)\n",
      "Processing class bird (3/10)\n",
      "Processing class cat (4/10)\n",
      "Processing class deer (5/10)\n",
      "Processing class dog (6/10)\n",
      "Processing class frog (7/10)\n",
      "Processing class horse (8/10)\n",
      "Processing class ship (9/10)\n",
      "Processing class truck (10/10)\n",
      "Accuracy: 0.5000\n",
      "F1 Score: 0.6667\n",
      "Confusion Matrix: \n",
      "tensor([[ 0., 30.],\n",
      "        [ 0., 30.]])\n",
      "AUROC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/30 [00:00<?, ?it/s]C:\\Users\\Bruger\\AppData\\Local\\Temp\\ipykernel_11104\\409205293.py:151: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_outputs = torch.tensor(batch_outputs)\n",
      "C:\\Users\\Bruger\\AppData\\Local\\Temp\\ipykernel_11104\\409205293.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_labels = torch.tensor(batch_labels, dtype=torch.int)\n",
      "Testing: 100%|██████████| 30/30 [00:00<00:00, 1034.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.6934\n",
      "Accuracy: 0.5000\n",
      "AUROC: 0.5000\n",
      "Confusion Matrix:\n",
      "tensor([[ 5., 25.],\n",
      "        [ 5., 25.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clip_pred(imgs_class, class_type, model, processor):\n",
    "    \"\"\"\n",
    "    Predict probabilities and extract features using the CLIP model.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=['A human-made photo of a ' + str(class_type), \n",
    "              'A synthetic computer-generated photo of a ' + str(class_type)],\n",
    "        images=imgs_class,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        img_features = model.get_image_features(pixel_values=inputs['pixel_values']).cpu()\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob[:, 1], img_features\n",
    "\n",
    "def load_images_from_folder(fake_folder, real_folder):\n",
    "    \"\"\"\n",
    "    Load images and classify them into real and fake.\n",
    "    \"\"\"\n",
    "    images_class = {'airplane': [[], []], 'automobile': [[], []], 'bird': [[], []], 'cat': [[], []],\n",
    "                    'deer': [[], []], 'dog': [[], []], 'frog': [[], []], 'horse': [[], []], \n",
    "                    'ship': [[], []], 'truck': [[], []]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', \n",
    "                   '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types[c]][0].append(img)\n",
    "                    images_class[class_types[c]][1].append(0)  # Real\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)  # Real\n",
    "\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types[c]][0].append(img)\n",
    "                    images_class[class_types[c]][1].append(1)  # Fake\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(1)  # Fake\n",
    "    return images_class\n",
    "\n",
    "def evaluate_model(imgs_class):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    probs, labels, image_features = [], [], []\n",
    "\n",
    "    for i, class_type in enumerate(imgs_class.keys()):\n",
    "        print(f\"Processing class {class_type} ({i + 1}/{len(imgs_class.keys())})\")\n",
    "        batch_probs, batch_features = clip_pred(imgs_class[class_type][0], class_type, model, processor)\n",
    "        probs.extend(batch_probs.cpu().tolist())\n",
    "        labels.extend(imgs_class[class_type][1])\n",
    "        image_features.extend(batch_features.cpu())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    probs = torch.tensor(probs)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    dataset = {'features': image_features, 'labels': labels}\n",
    "\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "\n",
    "    acc.update(probs, labels)\n",
    "    f1.update(probs, labels)\n",
    "    cm.update(probs, labels)\n",
    "    auroc.update(probs, labels)\n",
    "\n",
    "    print(f\"Accuracy: {acc.compute():.4f}\")\n",
    "    print(f\"F1 Score: {f1.compute():.4f}\")\n",
    "    print(f\"Confusion Matrix: \\n{cm.compute()}\")\n",
    "    print(f\"AUROC: {auroc.compute():.4f}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def train_loop(train_loader, model, loss, optimizer, num_epochs):\n",
    "    \"\"\"\n",
    "    Training loop for the feed-forward neural network.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    acc = BinaryAccuracy()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for batch_features, batch_labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            batch_features, batch_labels = batch_features.to(\"cuda\"), batch_labels.to(\"cuda\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_output = model(batch_features)\n",
    "            batch_loss = loss(batch_output.squeeze(), batch_labels)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc.update(batch_output.squeeze(), batch_labels)\n",
    "            epoch_acc += acc.compute().item()\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        avg_acc = epoch_acc / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
    "        acc.reset()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def test_network(test_loader, model, loss):\n",
    "    \"\"\"\n",
    "    Testing loop for the feed-forward neural network.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    acc = BinaryAccuracy()\n",
    "    auroc = BinaryAUROC()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            batch_features, batch_labels = batch_features.to(\"cuda\"), batch_labels.to(\"cuda\")\n",
    "            \n",
    "            batch_outputs = model(batch_features).squeeze()\n",
    "            batch_loss = loss(batch_outputs, batch_labels)\n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "            batch_outputs = torch.tensor(batch_outputs)\n",
    "             batch_labels = torch.tensor(batch_labels, dtype=torch.int)\n",
    "\n",
    "            acc.update(batch_outputs, batch_labels)\n",
    "            cm.update(batch_outputs, batch_labels)\n",
    "            auroc.update(batch_outputs, batch_labels)\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {acc.compute():.4f}\")\n",
    "    print(f\"AUROC: {auroc.compute():.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm.compute()}\")\n",
    "\n",
    "class dict_to_data(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "class network(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden):\n",
    "        super(network, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# File paths\n",
    "fake_train = r\"C:\\Users\\Bruger\\Documents\\School\\Feature-analysis-using-CLIP-model\\images\\fake_train\"\n",
    "real_train = r\"C:\\Users\\Bruger\\Documents\\School\\Feature-analysis-using-CLIP-model\\images\\real_train\"\n",
    "\n",
    "# Load training data\n",
    "imgs_class = load_images_from_folder(fake_train, real_train)\n",
    "feature_data_train = evaluate_model(imgs_class)\n",
    "train_dataset = dict_to_data(feature_data_train['features'], feature_data_train['labels'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=True)\n",
    "\n",
    "# Train the network\n",
    "num_features = len(train_dataset.features[0])\n",
    "ffnn = network(num_features, 256).to(\"cuda\")\n",
    "train_loop(train_loader, ffnn, nn.BCELoss(), optim.Adam(ffnn.parameters(), lr=1e-4), num_epochs=4)\n",
    "\n",
    "# Test the network\n",
    "fake_test = r\"C:\\Users\\Bruger\\Documents\\School\\Feature-analysis-using-CLIP-model\\images\\fake_test\"\n",
    "real_test = r\"C:\\Users\\Bruger\\Documents\\School\\Feature-analysis-using-CLIP-model\\images\\real_test\"\n",
    "test_imgs_class = load_images_from_folder(fake_test, real_test)\n",
    "feature_data_test = evaluate_model(test_imgs_class)\n",
    "test_dataset = dict_to_data(feature_data_test['features'], feature_data_test['labels'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True, drop_last=True)\n",
    "\n",
    "test_network(test_loader, ffnn, nn.BCELoss())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
