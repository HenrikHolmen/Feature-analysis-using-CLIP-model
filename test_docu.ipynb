{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot without classes incoporated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryPrecisionRecallCurve\n",
    "\n",
    "def clip_pred(imgs):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "        \n",
    "    \n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\DTU_repo\\deep_learning\\4_Convolutional\\images'\n",
    "images, labels = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images)\n",
    "metrics(probs[:,1], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC adjusted zero-shot no classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    \"\"\"\n",
    "    Load images and their corresponding labels from specified folders.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "    \n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "    \n",
    "    return images, torch.tensor(labels, dtype=torch.int)\n",
    "\n",
    "def evaluate_model(images, labels, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    \n",
    "    print(f\"Accuracy: {acc.compute().item():.4f}\")\n",
    "    print(f\"F1 Score: {f1.compute().item():.4f}\")\n",
    "    print(f\"Confusion Matrix: {cm.compute()}\")\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot with classes implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "def clip_pred(imgs_class, class_type, model, processor):\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text= ['A human-made photo of a' + str(class_type), 'A synthetic computer-generated photo of a' + str(class_type)],  # Prompts for each image\n",
    "        images=imgs_class,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob[:,1]\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images_class = {'airplane': [[],[]], 'automobile': [[],[]], 'bird': [[],[]], 'cat': [[],[]], 'deer': [[],[]], 'dog': [[],[]], 'frog': [[],[]], 'horse': [[],[]], 'ship': [[],[]], 'truck': [[],[]]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types.get(c)][0].append(img)\n",
    "                    images_class[class_types.get(c)][1].append(0)\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)\n",
    "\n",
    "    return images_class\n",
    "\n",
    "def evaluate_model(imgs_class):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cpu\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    labels = []\n",
    "    for i in imgs_class:\n",
    "        batch_probs = clip_pred(imgs_class[i][0], i,  model, processor)\n",
    "        probs.extend(batch_probs)\n",
    "        labels.extend(imgs_class[i][1])\n",
    "\n",
    "    probs = torch.tensor(probs)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(probs, labels)\n",
    "    f1.update(probs, labels)\n",
    "    cm.update(probs, labels)\n",
    "    auroc.update(probs, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "imgs_class = load_images_from_folder(folder_path)\n",
    "# Evaluate the model\n",
    "evaluate_model(imgs_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non HPC adjusted zero-shot with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "def clip_pred(imgs, imgs_class):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Pass the batch to the model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    auroc.update(ys, ts)\n",
    "\n",
    "    return acc.compute(), f1.compute(), cm.compute(), auroc.compute()\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images, imgs_class)\n",
    "metrics(probs[:, 1], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def clip_pred(imgs, imgs_class):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Pass the batch to the model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def evaluate_metrics(probs, labels):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "\n",
    "    acc.update(probs[:, 1], labels)\n",
    "    f1.update(probs[:, 1], labels)\n",
    "    cm.update(probs[:, 1], labels)\n",
    "    auroc.update(probs[:, 1], labels)\n",
    "\n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(labels.detach().numpy(), probs[:, 1].detach().numpy())\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, marker='.', label='ROC Curve (AUC = {:.2f})'.format(auroc_score))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images, imgs_class)\n",
    "evaluate_metrics(probs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted class code for HPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For running fake and real seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs_class, class_type, model, processor):\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text= ['A human-made photo of a' + str(class_type), 'A synthetic computer-generated photo of a' + str(class_type)],  # Prompts for each image\n",
    "        images=imgs_class,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.get_image_features(outputs['pixel_values'], hidden_states=False)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob[:,1], image_features\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images_class = {'airplane': [[],[]], 'automobile': [[],[]], 'bird': [[],[]], 'cat': [[],[]], 'deer': [[],[]], 'dog': [[],[]], 'frog': [[],[]], 'horse': [[],[]], 'ship': [[],[]], 'truck': [[],[]]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types.get(c)][0].append(img)\n",
    "                    images_class[class_types.get(c)][1].append(0)\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)\n",
    "\n",
    "    return images_class\n",
    "\n",
    "def evaluate_model(imgs_class):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cpu\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    labels = []\n",
    "    for i in imgs_class:\n",
    "        batch_probs = clip_pred(imgs_class[i][0], i,  model, processor)\n",
    "        probs.extend(batch_probs)\n",
    "        labels.extend(imgs_class[i][1])\n",
    "\n",
    "    probs = torch.tensor(probs)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(probs, labels)\n",
    "    f1.update(probs, labels)\n",
    "    cm.update(probs, labels)\n",
    "    auroc.update(probs, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "\n",
    "\n",
    "# Paths to the datasets\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "\n",
    "# Load the images and labels\n",
    "imgs_class= load_images_from_folder(folder_path)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(imgs_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake and real at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, imgs_class, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    return images, torch.tensor(labels, dtype=torch.int), imgs_class\n",
    "\n",
    "def evaluate_model(images, labels, imgs_class, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_classes = imgs_class[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, batch_classes, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    auroc.update(preds, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels, imgs_class = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels, imgs_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting image features and applying FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikto\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.13333334028720856\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix: \n",
      "tensor([[ 4., 26.],\n",
      "        [ 0.,  0.]])\n",
      "AUROC: 0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "def clip_pred(imgs_class, class_type, model, processor):\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text= ['A human-made photo of a' + str(class_type), 'A synthetic computer-generated photo of a' + str(class_type)],  # Prompts for each image\n",
    "        images=imgs_class,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cpu\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        img_features = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob[:,1], img_features\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images_class = {'airplane': [[],[]], 'automobile': [[],[]], 'bird': [[],[]], 'cat': [[],[]], 'deer': [[],[]], 'dog': [[],[]], 'frog': [[],[]], 'horse': [[],[]], 'ship': [[],[]], 'truck': [[],[]]}\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    images_class[class_types.get(c)][0].append(img)\n",
    "                    images_class[class_types.get(c)][1].append(0)\n",
    "                    break\n",
    "            if all(c not in filename for c in class_types.keys()):\n",
    "                images_class['airplane'][0].append(img)\n",
    "                images_class['airplane'][1].append(0)\n",
    "\n",
    "    return images_class\n",
    "\n",
    "def evaluate_model(imgs_class):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cpu\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    labels = []\n",
    "    image_features = []\n",
    "    for i in imgs_class:\n",
    "        batch_probs, batch_features = clip_pred(imgs_class[i][0], i,  model, processor)\n",
    "        probs.extend(batch_probs)\n",
    "        labels.extend(imgs_class[i][1])\n",
    "        for f in batch_features:\n",
    "            image_features.append(f)\n",
    "    \n",
    "    probs = torch.tensor(probs)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    dataset = {'features': image_features, 'labels': labels}\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(probs, labels)\n",
    "    f1.update(probs, labels)\n",
    "    cm.update(probs, labels)\n",
    "    auroc.update(probs, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "imgs_class = load_images_from_folder(folder_path)\n",
    "# Evaluate the model\n",
    "feature_data = evaluate_model(imgs_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed foward neural network on image features from encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class dict_to_data(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict_to_data(feature_data['features'], feature_data['labels'])\n",
    "train_loader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_features = 512\n",
    "num_hidden = 256\n",
    "\n",
    "class network(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_hidden):\n",
    "\n",
    "        super(network, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "ffnn = network(num_features, num_hidden)\n",
    "ffnn.to(\"cpu\")\n",
    "print(ffnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(ffnn.parameters(), lr=0.0001)\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.Tensor([0.7, 0.5, 0.1])\n",
    "t = torch.Tensor([1, 0, 1])\n",
    "preds = p > 0.5\n",
    "torch.sum(preds == t)/len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0875934511423111, Accuracy: 1.0\n",
      "Epoch 2, Loss: 0.08007360994815826, Accuracy: 1.0\n",
      "Epoch 3, Loss: 0.07435925304889679, Accuracy: 1.0\n",
      "Epoch 4, Loss: 0.0705740824341774, Accuracy: 1.0\n",
      "Epoch 5, Loss: 0.06528390944004059, Accuracy: 1.0\n",
      "Epoch 6, Loss: 0.05868624895811081, Accuracy: 1.0\n",
      "Epoch 7, Loss: 0.05837278813123703, Accuracy: 1.0\n",
      "Epoch 8, Loss: 0.05107269808650017, Accuracy: 1.0\n",
      "Epoch 9, Loss: 0.048593632876873016, Accuracy: 1.0\n",
      "Epoch 10, Loss: 0.04712023586034775, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ffnn.train()\n",
    "    acc = BinaryAccuracy()\n",
    "    for batch_features, batch_labels in train_loader:  \n",
    "        batch_features, batch_labels = batch_features.to(\"cpu\"), batch_labels.to(\"cpu\")\n",
    "\n",
    "        batch_output = ffnn(batch_features)\n",
    "        batch_loss = loss(batch_output.squeeze(), batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc.update(batch_output.squeeze(), batch_labels)\n",
    "        batch_accuracy = acc.compute()\n",
    "        acc.reset()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {batch_loss.item()}, Accuracy: {batch_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0459, Test Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0459), tensor(1.))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_network(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = BinaryAccuracy()\n",
    "    loss_func = nn.BCELoss()\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in test_loader:\n",
    "            batch_features, batch_labels = batch_features.to('cpu'), batch_labels.to('cpu')\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_features)\n",
    "            loss = loss_func(outputs.squeeze(), batch_labels)\n",
    "            test_loss += loss\n",
    "            test_acc.update(outputs.squeeze(), batch_labels)\n",
    "            accuracy = test_acc.compute()\n",
    "            \n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "test_network(ffnn, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
