{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot without classes incoporated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryPrecisionRecallCurve\n",
    "\n",
    "def clip_pred(imgs):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = clip_pred(images)\n",
    "probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "        \n",
    "    \n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\DTU_repo\\deep_learning\\4_Convolutional\\images'\n",
    "images, labels = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images)\n",
    "metrics(probs[:,1], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC adjusted zero-shot no classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=[\"a synthetic image created by AI\", \"a real image taken by a human\"],\n",
    "        images=imgs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "    prob = logits_per_image.softmax(dim=1)  # Probability over classes\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    \"\"\"\n",
    "    Load images and their corresponding labels from specified folders.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "    \n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "    \n",
    "    return images, torch.tensor(labels, dtype=torch.int)\n",
    "\n",
    "def evaluate_model(images, labels, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    \n",
    "    print(f\"Accuracy: {acc.compute().item():.4f}\")\n",
    "    print(f\"F1 Score: {f1.compute().item():.4f}\")\n",
    "    print(f\"Confusion Matrix: {cm.compute()}\")\n",
    "    return acc.compute(), f1.compute(), cm.compute()\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot with classes implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryPrecisionRecallCurve\n",
    "\n",
    "def clip_pred(imgs):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    logits = torch.empty(0,2)\n",
    "\n",
    "    for i in imgs:\n",
    "        img_class = imgs[i]\n",
    "        input = processor(\n",
    "            text=[\"a artificially generated image of a\" + str(i), \"a real image of a\" + str(i)],\n",
    "            images=img_class,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        output = model(**input)\n",
    "        logits = torch.cat((logits, output.logits_per_image), dim=0)  # this is the image-text similarity score\n",
    "    prob = logits.softmax(dim=1)  \n",
    "    return prob\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = {'airplane': [], 'automobile': [], 'bird': [], 'cat': [], 'deer': [], 'dog': [], 'frog': [], 'horse': [], 'ship': [], 'truck': []}\n",
    "    class_types = {'(2)':'automobile','(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        for key, value in class_types.items():\n",
    "                if key in filename:\n",
    "                     img_index = value\n",
    "                     break\n",
    "                else:\n",
    "                     img_index = 'airplane'\n",
    "        images[img_index].append(img)\n",
    "            \n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels\n",
    "\n",
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    prc = BinaryPrecisionRecallCurve()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    prc.update(ys, ts)\n",
    "\n",
    "    return acc.compute(), f1.compute(), cm.compute(), prc.compute()\n",
    "\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "images, labels = load_images_from_folder(folder_path)\n",
    "# probs = clip_pred(images)\n",
    "# metrics(probs[:,1], labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non HPC adjusted zero-shot with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "def clip_pred(imgs, imgs_class):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Pass the batch to the model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def metrics(ys, ts):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    acc.update(ys, ts)\n",
    "    f1.update(ys, ts)\n",
    "    cm.update(ys, ts)\n",
    "    auroc.update(ys, ts)\n",
    "\n",
    "    return acc.compute(), f1.compute(), cm.compute(), auroc.compute()\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images, imgs_class)\n",
    "metrics(probs[:, 1], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def clip_pred(imgs, imgs_class):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Pass the batch to the model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def evaluate_metrics(probs, labels):\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "\n",
    "    acc.update(probs[:, 1], labels)\n",
    "    f1.update(probs[:, 1], labels)\n",
    "    cm.update(probs[:, 1], labels)\n",
    "    auroc.update(probs[:, 1], labels)\n",
    "\n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(labels.detach().numpy(), probs[:, 1].detach().numpy())\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, marker='.', label='ROC Curve (AUC = {:.2f})'.format(auroc_score))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "folder_path = r'C:\\Users\\vikto\\Documents\\GitHub\\Feature-analysis-using-CLIP-model\\images'\n",
    "images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "probs = clip_pred(images, imgs_class)\n",
    "evaluate_metrics(probs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted class code for HPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For running fake and real seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, imgs_class, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename)).convert(\"RGB\")\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    labels = torch.zeros(len(images), dtype=torch.int)\n",
    "    return images, labels, imgs_class\n",
    "\n",
    "def evaluate_model(images, labels, imgs_class, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_classes = imgs_class[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, batch_classes, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    auroc.update(preds, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "# Paths to the datasets\n",
    "folder_path = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels, imgs_class = load_images_from_folder(folder_path)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels, imgs_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake and real at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, imgs_class, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    return images, torch.tensor(labels, dtype=torch.int), imgs_class\n",
    "\n",
    "def evaluate_model(images, labels, imgs_class, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_classes = imgs_class[i:i+batch_size]\n",
    "        batch_probs = clip_pred(batch_imgs, batch_classes, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    auroc.update(preds, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels, imgs_class = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels, imgs_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting image features and applying FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC\n",
    "\n",
    "# Enable debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"PYTHONFAULTHANDLER\"] = \"1\"\n",
    "\n",
    "# Log GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "    print(f\"CUDA Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your GPU setup.\")\n",
    "\n",
    "def clip_pred(imgs, imgs_class, model, processor):\n",
    "    \"\"\"\n",
    "    Perform prediction using the CLIP model.\n",
    "    \"\"\"\n",
    "    # Create prompts for all images\n",
    "    prompts = []\n",
    "    for img_class in imgs_class:\n",
    "        prompts.append(\"a synthetic image of a \" + str(img_class))\n",
    "        prompts.append(\"a real image of a \" + str(img_class))\n",
    "\n",
    "    # Process all images and prompts in a single batch\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=imgs * 2,  # Duplicate images to match the number of prompts\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        img_features = model.get_image_features(pixel_values = inputs['pixel_values'], output_hidden_states = False)\n",
    "    logits = outputs.logits_per_image  # this is the image-text similarity score\n",
    "\n",
    "    # Reshape logits to separate synthetic and real image logits\n",
    "    logits = logits.view(len(imgs), 2, -1).mean(dim=2)\n",
    "\n",
    "    prob = logits.softmax(dim=1)\n",
    "    return prob, img_features\n",
    "\n",
    "def load_images_from_folders(fake_folder, real_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_types = {'(2)': 'automobile', '(3)': 'bird', '(4)': 'cat', '(5)': 'deer', '(6)': 'dog', '(7)': 'frog', '(8)': 'horse', '(9)': 'ship', '(10)': 'truck'}\n",
    "    imgs_class = []\n",
    "\n",
    "    # Load FAKE images\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        img_path = os.path.join(fake_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(0)  # Label for \"FAKE\"\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    # Load REAL images\n",
    "    for filename in os.listdir(real_folder):\n",
    "        img_path = os.path.join(real_folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(img)\n",
    "            labels.append(1)  # Label for \"REAL\"\n",
    "\n",
    "            for c in class_types.keys():\n",
    "                if c in filename:\n",
    "                    imgs_class.append(class_types.get(c))\n",
    "                    break\n",
    "            else:\n",
    "                imgs_class.append('airplane')\n",
    "\n",
    "    return images, torch.tensor(labels, dtype=torch.int), imgs_class\n",
    "\n",
    "def evaluate_model(images, labels, imgs_class, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model using mini-batch processing and calculate metrics.\n",
    "    \"\"\"\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Process images in mini-batches\n",
    "    probs = []\n",
    "    image_features = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_imgs = images[i:i+batch_size]\n",
    "        batch_classes = imgs_class[i:i+batch_size]\n",
    "        batch_probs, batch_features = clip_pred(batch_imgs, batch_classes, model, processor)\n",
    "        probs.append(batch_probs)\n",
    "        image_features.append(batch_features)\n",
    "    \n",
    "    probs = torch.cat(probs, dim=0)  # Combine all batches\n",
    "    preds = torch.argmax(probs, dim=1)  # Predicted labels\n",
    "    image_features = torch.cat(batch_features, dim=0)\n",
    "    #TODO torch.save?\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = BinaryAccuracy()\n",
    "    f1 = BinaryF1Score()\n",
    "    cm = BinaryConfusionMatrix()\n",
    "    auroc = BinaryAUROC()\n",
    "    \n",
    "    acc.update(preds, labels)\n",
    "    f1.update(preds, labels)\n",
    "    cm.update(preds, labels)\n",
    "    auroc.update(preds, labels)\n",
    "    \n",
    "    accuracy = acc.compute()\n",
    "    f1_score = f1.compute()\n",
    "    confusion_matrix = cm.compute()\n",
    "    auroc_score = auroc.compute()\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\")\n",
    "    print(f\"AUROC: {auroc_score}\")\n",
    "\n",
    "    #Create dataset with labels and features for FFNN\n",
    "    #TODO\n",
    "\n",
    "\n",
    "# Paths to the datasets\n",
    "fake_folder = r'/dtu/blackhole/18/160664/test/FAKE/'\n",
    "real_folder = r'/dtu/blackhole/18/160664/test/REAL/'\n",
    "\n",
    "# Load the images and labels\n",
    "images, labels, imgs_class = load_images_from_folders(fake_folder, real_folder)\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(images, labels, imgs_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed foward neural network on image features from encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class network(nn.Module):\n",
    "\n",
    "    def __init__(self, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
